{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPI 270   \n",
    "\n",
    "# Beginner DBDP Tutorial: Working with Continuous Glucose Monitoring (CGM) Data\n",
    "# Credit: Brinnae Bent, PhD\n",
    "\n",
    "\n",
    "In this case study, we will be using continuous glucose monitoring (CGM) data. We will be performing the first few steps of the Digital Biomarker Discvoery Pipeline: \n",
    "\n",
    "1. Pre-process the data\n",
    "2. Exploratory data analysis\n",
    "3. Feature Engineering (using the DBDP cgmquantify module)\n",
    "\n",
    "### Level\n",
    "Beginner. You have limited knowledge of Python/R. This tutorial is a step-by-step guide.\n",
    "\n",
    "### System Requirements\n",
    "* Python (3.0.0+) - If you don't yet have Python installed, please follow [this video tutorial](https://www.youtube.com/watch?v=YJC6ldI3hWk) on how to download Python through Anaconda\n",
    "\n",
    "For beginners, I recommend using either JupyterLab or Visual Studio Code to interface with your notebooks.\n",
    "\n",
    "### Objectives\n",
    "* Learn how to work with CGM data\n",
    "* Learn the first 3 steps of the digital biomarker discovery pipeline\n",
    "* Compute glucose variability metrics on data\n",
    "\n",
    "### Dataset\n",
    "We will be using a small example dataset from a single person's week long CGM data. This data has been de-identified and time shifted.\n",
    "\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries \n",
    "If you installed with Anaconda, some of these libaries should be built-in. \n",
    "\n",
    "You can check if you have them by running the next block of code.\n",
    "\n",
    "If you get an error, install them in your terminal window using \"pip install X\" where X is the package missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Import and Clean CGM Data\n",
    "In python, functions are used by library.function. Below, we are using the pandas library (which we nicknamed \"pd\" when we imported it above). We want to use the read_csv function within the pandas library.\n",
    "\n",
    "If you added the data-set in your local folder (the one your code is in), you can proceed. You may need to change the filename to be your entire folder path and filename if you have the dataset elsewhere.\n",
    "\n",
    "We are importing the data as a special type of data structure, the dataframe. Learn more about data structures and, in particular, dataframes [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_file.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the data is pretty messy. For today's purposes, we only care about the Timestamp and Glucose columns. We want to drop everything else from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new dataframe. We will call this one \"df\"\n",
    "df = pd.DataFrame() # This creates an empty data frame\n",
    "\n",
    "# Pull timestamp and glucose columns into the new, empty dataframe. We will change their names to \"DateTime\" and \"Glucose\" \n",
    "df['DateTime'] = data['Timestamp (YYYY-MM-DDThh:mm:ss)']\n",
    "# For glucose, we also want to change the data to a number. Right now, Python thinks the glucose column values are strings (you can think of a string as text). We will wrap the function in pd.to_numeric() to convert the column to numbers.\n",
    "df['Glucose'] = pd.to_numeric(data['Glucose Value (mg/dL)'])\n",
    "\n",
    "# The first 12 rows don't even have matching glucose + time values, so let's drop those\n",
    "df.drop(df.index[:12], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a clean dataframe to work with. Check it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deal with the DateTime. Remember how we had to change the glucose column to be numbers instead of strings? \n",
    "\n",
    "Well, we have to tell Python that the \"DateTime\" column is a datetime data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use pd.to_datetime() to convert the Time column to a datetime object. We need to tell Python what the current format is. \n",
    "df['DateTime'] =  pd.to_datetime(df['DateTime'], format='%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# I would also like to make a new column, \"Day\", with just the Day of the DateTime column.\n",
    "df['Day'] = df['DateTime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that your index (the bold numbers on the left side of the dataframe) starts at 12? That is because we removed the first 12 rows from the dataframe. Let's re-index the dataframe so the index will start at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Exploratory Data Analysis\n",
    "We will explore just a few components of Exploratory Data Analysis with CGM data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a brief overview of our data, running the describe() function on our dataframe will give us a nice summary, including count, mean, standard deviation, median, minimum, and maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple plot, so I can start to understand what is happening to my data over time. This will also enable me to see missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate a figure size and font size\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Plot\n",
    "plt.plot(df['DateTime'], df['Glucose'], '.', color = '#1f77b4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have a little bit of missing data, on 10-26 and on 10-31. That's an important thing to address, but not our focus today. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glucose excursions (very high and very low glucose values) are important for assessing glycemic health. Let's re-plot this, but including the mean and standard deviation of glucose on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of glucose. We will draw 3 lines on the plot: The mean, 1 standard deviation above the mean, and 1 standard deviation below the mean.\n",
    "glucose_mean = np.mean(df['Glucose'])\n",
    "up = np.mean(df['Glucose']) + np.std(df['Glucose'])\n",
    "dw = np.mean(df['Glucose']) - np.std(df['Glucose'])\n",
    "\n",
    "# Same plot as above\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.plot(df['DateTime'], df['Glucose'], '.', color = '#1f77b4')\n",
    "\n",
    "# Plot 3 horizontal lines\n",
    "plt.axhline(y=glucose_mean, color='red', linestyle='-')\n",
    "plt.axhline(y=up, color='pink', linestyle='-')\n",
    "plt.axhline(y=dw, color='pink', linestyle='-')\n",
    "\n",
    "# Add a label\n",
    "plt.ylabel('Glucose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we wanted to see a smoothed graph that performed interpolation between points. We can use lowess smoothing to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lowess from the statsmodels library\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a new data frame that is the smoothed version of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteres = lowess(df['Glucose'], df['DateTime'], is_sorted=True, frac=0.015, it=0) #0.025\n",
    "filtered = pd.to_datetime(filteres[:,0], format='%Y-%m-%dT%H:%M:%S') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the smoothed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sizes\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Same plot as before\n",
    "plt.plot(df['DateTime'], df['Glucose'], '.')\n",
    "\n",
    "# Plot smoothed data\n",
    "plt.plot(filtered, filteres[:,1], 'r')\n",
    "\n",
    "#Labels\n",
    "plt.ylabel('Glucose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more or less smoothing, you can change the \"frac\" argument in the lowess function above. It is currently set at 0.015. Why not try 0.03 and see what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "In order to determine the distribution of Glucose in the dataset, let's plot a histogram using the matplotlib hist function.\n",
    "\n",
    "For a list of all of the arguments to the matplotlib hist() function, check out the documentation [here](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sizes\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(df['Glucose'])\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is the process by which we engineer features that we can then use as inputs in models to predict an outcome. \n",
    "\n",
    "The process of Feature Engineering can be data-driven or domain-driven. We will do a couple of each here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data-driven Feature Engineering\n",
    "\n",
    "Simple data-driven features include summary metrics. Let's compute those below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanG = np.nanmean(df['Glucose'])\n",
    "medianG = np.nanmedian(df['Glucose'])\n",
    "minG = np.nanmin(df['Glucose'])\n",
    "maxG = np.nanmax(df['Glucose'])\n",
    "Q1G = np.nanpercentile(df['Glucose'], 25)\n",
    "Q3G = np.nanpercentile(df['Glucose'], 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute metrics over each day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation over all days\n",
    "interdaysd = np.std(df['Glucose'])\n",
    "\n",
    "# Standard Deviation for each day\n",
    "intradaysd =[]\n",
    "\n",
    "for i in pd.unique(df['Day']):\n",
    "    intradaysd.append(np.std(df[df['Day']==i][\"Glucose\"]))\n",
    "    \n",
    "# We can find the average intraday standard deviation:\n",
    "intradaysd_mean = np.mean(intradaysd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine both the interdaysd and the intradaysd_mean, we see that they are different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intradaysd_mean)\n",
    "print(interdaysd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain-driven Feature Engineering\n",
    "\n",
    "Domain-driven feature engineering usually takes the most time but is often the most effective, especially when handling physiology-based metrics. Domain-driven feature engineering involves a literature review and a better understanding of the underlying physiology. \n",
    "\n",
    "Two metrics that are well known in diabetes literature to be indicative of glycemic health are the [glucose management index (GMI)](https://care.diabetesjournals.org/content/early/2018/09/17/dc18-1581) and the [J-index](https://diabetes.diabetesjournals.org/content/62/5/1398). Let's compute these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 0.001*((np.mean(df['Glucose'])+np.std(df['Glucose']))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMI = 3.31 + (0.02392*np.mean(df['Glucose']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "The DBDP CGMQuantify Module has 25+ pre-configured features that it computes. [cgmquanitfy module](https://github.com/DigitalBiomarkerDiscoveryPipeline/cgmquantify). The code is available [here](https://github.com/DigitalBiomarkerDiscoveryPipeline/cgmquantify/blob/master/cgmquantify/__init__.py).\n",
    "\n",
    "You can also install the cgmquantify package and have access to its functions. (Remember to pip install the package. For full instructions on how to install it, see [here](https://github.com/DigitalBiomarkerDiscoveryPipeline/cgmquantify/blob/master/README.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgmquantify as cgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cgm.importdexcom('test_file.csv')\n",
    "\n",
    "print('interdaysd is: ' + str(cgm.interdaysd(data)))\n",
    "print('interdaycv is: ' + str(cgm.interdaycv(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB QUESTION 1:\n",
    "\n",
    "Do a literature search for eA1C and describe what it is, and why it may be a meaningful feature for these types of data. \n",
    "\n",
    "Calculate the eA1C for the sample. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB QUESTION 2:\n",
    "\n",
    "Perform a literature search for the time in range (TIR) and time out of range (TOR) for diabetes and describe why this is a meaningful feature in diabetes management. \n",
    "\n",
    "Calculate the ratio of time in range (TIR) vs time out of range (TOR) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB QUESTION 3:\n",
    "\n",
    "Perform a literature search related to continuous glucose monitoring and identify another feature that may be interesting to researchers or clinicians. Describe that features, and how it's calculated, and then implement the calulation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB EXERCISE \n",
    "\n",
    "This exercise can be done in any software you choose (cgmquantify is available on R), including in this Jupyter notebook. Keep in mind though, your instance will only last until it times out from non-use, or you close your browser. If you save your changes, you'll need to download the ipynb and then reupload it for it to work properly. \n",
    "\n",
    "Download the dataset located here: https://public.jaeb.org/dataset/559\n",
    "\n",
    "The file you'll be interested in is entitled NonDiabDeviceCGM.csv\n",
    "\n",
    "This is a dataset of CGM measurements from 200 non-diabetic individuals for up to 46 days. Your task is to use a new feature to group these participants into groups based on any hypothesis. You don't need to prove they are different, or run any statistical tests, just group them and then describe those groups using a few plots and visualizations. You can use any features from the CGMQuantify module, or any new features that you develop related to blood glucose levels. The zip file that you download will include many other files which describe the age, weight, height, gender, race, ethnicity, medications and pre-existing conditions of the participants Feel free to join these tables to your dataset and include these in your analysis, or even as the subject of your grouping. \n",
    "\n",
    "## Following the exercise, write a data reflection, addressing the following questions:\n",
    "\n",
    "Proposed research question\n",
    "Initial findings in data exploration (mean, median, SD, skew, etc.)\n",
    "Overview of your Feature (why did you choose it? what does it tell you? how is it calculated?)\n",
    "Description and rationale for your grouping\n",
    "Observations from your analysis\n",
    "Challenges in using the dataset\n",
    "Conclusion on your research question\n",
    "Related tables and figures\n",
    "Copy of annotated code (feel free to print the Jupyter notebook)\n",
    "Answers and code from the Lab Questions above. \n",
    "\n",
    "### Other Recommended Resources\n",
    "* [Paper on glucose variability](https://journals.sagepub.com/doi/full/10.1177/1932296819826111)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
